{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MyTokenizer","text":"<ul> <li> <p> Python 3.8+     ---     Works with all modern Python versions</p> </li> <li> <p> High Performance     ---     Optimized for speed and memory efficiency</p> </li> <li> <p> Multiple Algorithms     ---     Supports BPE, WordPiece, and Unigram</p> </li> <li> <p> Production Ready     ---     Battle-tested and ready for deployment</p> </li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>MyTokenizer is a high-performance, production-ready tokenizer that supports multiple tokenization algorithms including Byte Pair Encoding (BPE), WordPiece, and Unigram. Designed with both simplicity and performance in mind, it's perfect for natural language processing tasks where tokenization speed and accuracy are critical.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multiple Tokenization Algorithms: Choose between BPE, WordPiece, or Unigram based on your needs</li> <li>Blazing Fast: Optimized implementation for maximum performance</li> <li>Memory Efficient: Smart memory management for large datasets</li> <li>Easy to Use: Simple, intuitive API with sensible defaults</li> <li>Production Ready: Thoroughly tested and production-hardened</li> <li>Extensible: Easy to extend with custom tokenization logic</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from tokenizer import Tokenizer\n\n# Initialize with default settings\ntokenizer = Tokenizer()\n\n# Tokenize some text\ntokens = tokenizer.tokenize(\"Hello, world! This is MyTokenizer in action.\")\nprint(tokens)\n# Output: ['Hello', ',', 'world', '!', 'This', 'is', 'My', '##Token', '##izer', 'in', 'action', '.']\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Install MyTokenizer</li> <li>Quickstart - Get up and running in minutes</li> <li>User Guide - Learn how to use all features</li> <li>API Reference - Detailed API documentation</li> </ol>"},{"location":"#community","title":"Community","text":"<ul> <li>GitHub Issues: Found a bug or have a feature request? Open an issue</li> </ul>"},{"location":"#support-and-community","title":"Support and Community","text":"<p>If you need help or have questions, please open an issue. For commercial support, please contact pranav.singh01010101@gmail.com.</p>"},{"location":"#license-and-contributing","title":"License and Contributing","text":"<p>This project is licensed under the MIT License. The source code is available on GitHub. We welcome contributions from the community!</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use MyTokenizer in your research, please consider citing it:</p> <p>```bibtex @software{MyTokenizer,   author = {Pranav Singh, Raman Mendiratta},   title = {MyTokenizer: A High-Performance Tokenizer for NLP},   year = {2025},   publisher = {GitHub},   journal = {GitHub repository},   howpublished = {\\url{https://github.com/pranav271103/MyTokenizer}} }</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#using-pip","title":"Using pip","text":"<pre><code>pip install mytokenizer\n</code></pre>"},{"location":"installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/pranav271103/MyTokenizer.git\ncd MyTokenizer\npip install -e .\n</code></pre>"},{"location":"installation/#quickstart","title":"Quickstart","text":"<pre><code>from tokenizer import Tokenizer\n\n# Initialize tokenizer\ntokenizer = Tokenizer()\n\n# Tokenize text\ntokens = tokenizer.tokenize(\"Hello, world!\")\nprint(tokens)\n</code></pre>"},{"location":"api/tokenizer/","title":"Tokenizer API Reference","text":"<p>This document provides detailed documentation for the <code>Tokenizer</code> class, which is the main class for tokenizing text using various algorithms.</p>"},{"location":"api/tokenizer/#class-tokenizer","title":"Class: Tokenizer","text":"<pre><code>class tokenizer.Tokenizer(config=None, **kwargs)\n</code></pre> <p>Main class for tokenizing text. This class can be configured to use different tokenization algorithms (BPE, WordPiece, Unigram) and supports various customization options.</p>"},{"location":"api/tokenizer/#parameters","title":"Parameters","text":"<ul> <li>config (<code>TokenizerConfig</code>, optional) - Configuration object for the tokenizer. If not provided, default settings will be used.</li> <li>**kwargs - Additional keyword arguments that will be used to update the configuration.</li> </ul>"},{"location":"api/tokenizer/#attributes","title":"Attributes","text":"<ul> <li>vocab (<code>Vocabulary</code>) - The vocabulary used by the tokenizer.</li> <li>config (<code>TokenizerConfig</code>) - The configuration object for the tokenizer.</li> <li>preprocessor (<code>callable</code>, optional) - Function to preprocess text before tokenization.</li> <li>postprocessor (<code>callable</code>, optional) - Function to postprocess tokens after tokenization.</li> </ul>"},{"location":"api/tokenizer/#methods","title":"Methods","text":""},{"location":"api/tokenizer/#tokenize","title":"<code>tokenize</code>","text":"<pre><code>tokenize(text, **kwargs)\n</code></pre> <p>Tokenize the input text.</p> <p>Parameters: - text (<code>str</code>) - The text to tokenize. - **kwargs - Additional arguments to override tokenizer configuration.</p> <p>Returns: - <code>List[str]</code> - List of tokens.</p> <p>Example: <pre><code>tokenizer = Tokenizer()\ntokens = tokenizer.tokenize(\"Hello, world!\")\n# Returns: [\"Hello\", \",\", \"world\", \"!\"]\n</code></pre></p>"},{"location":"api/tokenizer/#tokenize_batch","title":"<code>tokenize_batch</code>","text":"<pre><code>tokenize_batch(texts, **kwargs)\n</code></pre> <p>Tokenize a batch of texts.</p> <p>Parameters: - texts (<code>List[str]</code>) - List of texts to tokenize. - **kwargs - Additional arguments to override tokenizer configuration.</p> <p>Returns: - <code>List[List[str]]</code> - List of token lists, one for each input text.</p> <p>Example: <pre><code>tokenizer = Tokenizer()\ntexts = [\"First text\", \"Second text\"]\nbatch_tokens = tokenizer.tokenize_batch(texts)\n# Returns: [[\"First\", \"text\"], [\"Second\", \"text\"]]\n</code></pre></p>"},{"location":"api/tokenizer/#train","title":"<code>train</code>","text":"<pre><code>train(files, vocab_size=30000, min_frequency=2, show_progress=True, **kwargs)\n</code></pre> <p>Train the tokenizer on the given files.</p> <p>Parameters: - files (<code>str</code> or <code>List[str]</code>) - File path or list of file paths to train on. - vocab_size (<code>int</code>, optional, defaults to 30000) - Maximum size of the vocabulary. - min_frequency (<code>int</code>, optional, defaults to 2) - Minimum frequency for a token to be included. - show_progress (<code>bool</code>, optional, defaults to <code>True</code>) - Whether to show a progress bar. - **kwargs - Additional training parameters.</p> <p>Example: <pre><code>tokenizer = Tokenizer()\ntokenizer.train(\"data.txt\", vocab_size=50000)\n</code></pre></p>"},{"location":"api/tokenizer/#save","title":"<code>save</code>","text":"<pre><code>save(file_path)\n</code></pre> <p>Save the tokenizer to a file.</p> <p>Parameters: - file_path (<code>str</code>) - Path where to save the tokenizer.</p> <p>Example: <pre><code>tokenizer.save(\"models/my_tokenizer.model\")\n</code></pre></p>"},{"location":"api/tokenizer/#load","title":"<code>load</code>","text":"<pre><code>@classmethod\nload(file_path)\n</code></pre> <p>Load a tokenizer from a file.</p> <p>Parameters: - file_path (<code>str</code>) - Path to the saved tokenizer file.</p> <p>Returns: - <code>Tokenizer</code> - The loaded tokenizer instance.</p> <p>Example: <pre><code>tokenizer = Tokenizer.load(\"models/pretrained.model\")\n</code></pre></p>"},{"location":"api/tokenizer/#add_special_tokens","title":"<code>add_special_tokens</code>","text":"<pre><code>add_special_tokens(tokens)\n</code></pre> <p>Add special tokens to the vocabulary.</p> <p>Parameters: - tokens (<code>List[str]</code> or <code>str</code>) - Token or list of tokens to add.</p> <p>Example: <pre><code>tokenizer.add_special_tokens([\"[NEW1]\", \"[NEW2]\"])\n</code></pre></p>"},{"location":"api/tokenizer/#add_special_case","title":"<code>add_special_case</code>","text":"<pre><code>add_special_case(token, tokens_sequence)\n</code></pre> <p>Add a special case for tokenization.</p> <p>Parameters: - token (<code>str</code>) - The token to replace. - tokens_sequence (<code>List[str]</code>) - The sequence of tokens to replace it with.</p> <p>Example: <pre><code>tokenizer.add_special_case(\"gonna\", [\"gon\", \"na\"])\n</code></pre></p>"},{"location":"api/tokenizer/#enable_cache","title":"<code>enable_cache</code>","text":"<pre><code>enable_cache(max_size=10000)\n</code></pre> <p>Enable caching of tokenization results.</p> <p>Parameters: - max_size (<code>int</code>, optional, defaults to 10000) - Maximum number of items to cache.</p>"},{"location":"api/tokenizer/#disable_cache","title":"<code>disable_cache</code>","text":"<pre><code>disable_cache()\n</code></pre> <p>Disable caching of tokenization results.</p>"},{"location":"api/tokenizer/#clear_cache","title":"<code>clear_cache</code>","text":"<pre><code>clear_cache()\n</code></pre> <p>Clear the tokenization cache.</p>"},{"location":"api/tokenizer/#properties","title":"Properties","text":""},{"location":"api/tokenizer/#vocab_size","title":"<code>vocab_size</code>","text":"<pre><code>@property\nvocab_size()\n</code></pre> <p>Get the size of the vocabulary.</p> <p>Returns: - <code>int</code> - Number of tokens in the vocabulary.</p>"},{"location":"api/tokenizer/#special_tokens","title":"<code>special_tokens</code>","text":"<pre><code>@property\nspecial_tokens()\n</code></pre> <p>Get the list of special tokens.</p> <p>Returns: - <code>List[str]</code> - List of special tokens.</p>"},{"location":"api/tokenizer/#class-methods","title":"Class Methods","text":""},{"location":"api/tokenizer/#from_pretrained","title":"<code>from_pretrained</code>","text":"<pre><code>@classmethod\nfrom_pretrained(model_name_or_path, **kwargs)\n</code></pre> <p>Load a pretrained tokenizer.</p> <p>Parameters: - model_name_or_path (<code>str</code>) - Name or path of the pretrained model. - **kwargs - Additional arguments for loading the tokenizer.</p> <p>Returns: - <code>Tokenizer</code> - The loaded tokenizer instance.</p> <p>Example: <pre><code>tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n</code></pre></p>"},{"location":"api/tokenizer/#get_config","title":"<code>get_config</code>","text":"<pre><code>@classmethod\nget_config()\n</code></pre> <p>Get the default configuration for the tokenizer.</p> <p>Returns: - <code>dict</code> - Default configuration dictionary.</p>"},{"location":"api/tokenizer/#configuration","title":"Configuration","text":"<p>The <code>Tokenizer</code> class can be configured using a <code>TokenizerConfig</code> object. Here are the available configuration options:</p> <pre><code>config = TokenizerConfig(\n    algorithm=\"bpe\",          # or \"wordpiece\", \"unigram\"\n    vocab_size=30000,         # Maximum vocabulary size\n    min_frequency=2,          # Minimum token frequency\n    lowercase=True,           # Convert to lowercase\n    strip_accents=True,       # Strip accents\n    max_length=512,           # Maximum sequence length\n    truncation=True,          # Whether to truncate sequences\n    padding=\"max_length\",     # Padding strategy\n    special_tokens={\n        \"unk_token\": \"[UNK]\",\n        \"pad_token\": \"[PAD]\"\n    }\n)\n</code></pre>"},{"location":"api/tokenizer/#examples","title":"Examples","text":""},{"location":"api/tokenizer/#basic-usage","title":"Basic Usage","text":"<pre><code>from tokenizer import Tokenizer\n\n# Initialize with default settings\ntokenizer = Tokenizer()\n\n# Tokenize text\ntokens = tokenizer.tokenize(\"Hello, world!\")\nprint(tokens)\n# Output: [\"Hello\", \",\", \"world\", \"!\"]\n</code></pre>"},{"location":"api/tokenizer/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Custom configuration\nconfig = TokenizerConfig(\n    algorithm=\"wordpiece\",\n    vocab_size=50000,\n    lowercase=True,\n    special_tokens={\n        \"unk_token\": \"[UNK]\",\n        \"pad_token\": \"[PAD]\"\n    }\n)\n\n# Initialize with custom configuration\ntokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"api/tokenizer/#training-a-tokenizer","title":"Training a Tokenizer","text":"<pre><code>from tokenizer import Tokenizer\n\n# Initialize tokenizer\ntokenizer = Tokenizer()\n\n# Train on a text file\ntokenizer.train(\"data.txt\", vocab_size=50000)\n\n# Save the trained tokenizer\ntokenizer.save(\"my_tokenizer.model\")\n\n# Load the tokenizer later\nloaded_tokenizer = Tokenizer.load(\"my_tokenizer.model\")\n</code></pre>"},{"location":"api/tokenizer/#notes","title":"Notes","text":"<ul> <li>The tokenizer automatically handles unknown tokens by replacing them with the <code>[UNK]</code> token.</li> <li>Special tokens are never split during tokenization.</li> <li>The tokenizer can be extended with custom preprocessors and postprocessors.</li> </ul>"},{"location":"examples/finetuning/","title":"Fine-tuning Examples","text":"<p>This guide demonstrates how to fine-tune pre-trained language models using the tokenizer.</p>"},{"location":"examples/finetuning/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Fine-tuning BERT</li> <li>Fine-tuning RoBERTa</li> <li>Fine-tuning GPT-2</li> <li>Custom Dataset with DataLoader</li> <li>Training Loop with PyTorch Lightning</li> <li>Saving and Loading Fine-tuned Models</li> </ul>"},{"location":"examples/finetuning/#fine-tuning-bert","title":"Fine-tuning BERT","text":"<pre><code>import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW\nfrom tokenizer import Tokenizer\n\n# Load pre-trained tokenizer\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Prepare dataset\ntexts = [\"This is a positive example.\", \"This is a negative example.\"]\nlabels = [1, 0]\n\n# Tokenize inputs\ninputs = tokenizer.encode_batch(texts, padding=True, truncation=True, return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\nlabels = torch.tensor(labels)\n\n# Create DataLoader\ndataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\ndataloader = DataLoader(dataset, batch_size=2)\n\n# Load pre-trained model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# Set up optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training loop\nmodel.train()\nfor epoch in range(3):  # Number of epochs\n    for batch in dataloader:\n        batch_input_ids, batch_attention_mask, batch_labels = batch\n\n        # Forward pass\n        outputs = model(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask,\n            labels=batch_labels\n        )\n\n        # Backward pass\n        loss = outputs.loss\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(f\"Loss: {loss.item()}\")\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"models/finetuned_bert\")\ntokenizer.save(\"models/finetuned_bert/tokenizer.model\")\n</code></pre>"},{"location":"examples/finetuning/#fine-tuning-roberta","title":"Fine-tuning RoBERTa","text":"<pre><code>from transformers import RobertaForSequenceClassification, AdamW\nfrom tokenizer import Tokenizer\nimport torch\n\n# Load RoBERTa tokenizer\ntokenizer = Tokenizer.from_pretrained(\"roberta-base\")\n\n# Prepare dataset\ntexts = [\"This is the first example.\", \"This is the second example.\"]\nlabels = [1, 0]\n\n# Tokenize inputs\ninputs = tokenizer.encode_batch(\n    texts,\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\n# Create DataLoader\ninput_ids = inputs[\"input_ids\"]\nattention_mask = inputs[\"attention_mask\"]\nlabels = torch.tensor(labels)\n\ndataset = torch.utils.data.TensorDataset(input_ids, attention_mask, labels)\ndataloader = DataLoader(dataset, batch_size=2)\n\n# Load RoBERTa model\nmodel = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n\n# Set up optimizer and training parameters\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Training loop\nmodel.train()\nfor epoch in range(3):\n    for batch in dataloader:\n        batch_input_ids, batch_attention_mask, batch_labels = batch\n\n        outputs = model(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask,\n            labels=batch_labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n# Save the model\nmodel.save_pretrained(\"models/finetuned_roberta\")\ntokenizer.save(\"models/finetuned_roberta/tokenizer.model\")\n</code></pre>"},{"location":"examples/finetuning/#fine-tuning-gpt-2","title":"Fine-tuning GPT-2","text":"<pre><code>from transformers import GPT2LMHeadModel, AdamW\nfrom tokenizer import Tokenizer\nimport torch\n\n# Load GPT-2 tokenizer\ntokenizer = Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\n# Prepare dataset\ntexts = [\n    \"In this tutorial, we will learn how to\",\n    \"The quick brown fox jumps over\"\n]\n\n# Tokenize inputs\ninputs = tokenizer.encode_batch(\n    texts,\n    padding=True,\n    truncation=True,\n    max_length=128,\n    return_tensors=\"pt\"\n)\n\n# Shift inputs for language modeling\ninput_ids = inputs[\"input_ids\"]\nlabels = input_ids.clone()\n\n# Create DataLoader\ndataset = torch.utils.data.TensorDataset(input_ids, labels)\ndataloader = DataLoader(dataset, batch_size=2)\n\n# Load GPT-2 model\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer))  # Update for new tokens\n\n# Set up optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\nmodel.train()\nfor epoch in range(3):\n    for batch in dataloader:\n        batch_input_ids, batch_labels = batch\n\n        outputs = model(\n            input_ids=batch_input_ids,\n            labels=batch_labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n\n# Save the model\nmodel.save_pretrained(\"models/finetuned_gpt2\")\ntokenizer.save(\"models/finetuned_gpt2/tokenizer.model\")\n</code></pre>"},{"location":"examples/finetuning/#custom-dataset-with-dataloader","title":"Custom Dataset with DataLoader","text":"<pre><code>from torch.utils.data import Dataset, DataLoader\nfrom tokenizer import Tokenizer\nimport torch\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Example usage\ntexts = [\"This is a positive review.\", \"This is a negative review.\"]\nlabels = [1, 0]\n\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\ndataset = TextClassificationDataset(texts, labels, tokenizer)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n# Example training loop\nmodel = ...  # Your model here\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\nfor epoch in range(3):\n    for batch in dataloader:\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['label']\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(f\"Loss: {loss.item()}\")\n</code></pre>"},{"location":"examples/finetuning/#training-loop-with-pytorch-lightning","title":"Training Loop with PyTorch Lightning","text":"<pre><code>import pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AdamW\nfrom tokenizer import Tokenizer\n\nclass TextClassifier(pl.LightningModule):\n    def __init__(self, model_name=\"bert-base-uncased\", num_labels=2, learning_rate=2e-5):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Load pre-trained model\n        self.model = ...  # Your model here\n        self.tokenizer = Tokenizer.from_pretrained(model_name)\n\n        # Metrics\n        self.train_acc = pl.metrics.Accuracy()\n        self.val_acc = pl.metrics.Accuracy()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['label']\n        )\n\n        loss = outputs.loss\n        preds = torch.argmax(outputs.logits, dim=1)\n        acc = self.train_acc(preds, batch['label'])\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch['input_ids'],\n            attention_mask=batch['attention_mask'],\n            labels=batch['label']\n        )\n\n        loss = outputs.loss\n        preds = torch.argmax(outputs.logits, dim=1)\n        acc = self.val_acc(preds, batch['label'])\n\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        return AdamW(self.parameters(), lr=self.hparams.learning_rate)\n\n# Example usage\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = TextClassifier()\n\n# Prepare data loaders\ntrain_dataset = ...  # Your dataset here\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Train the model\ntrainer = pl.Trainer(\n    max_epochs=3,\n    gpus=1 if torch.cuda.is_available() else 0,\n    progress_bar_refresh_rate=10\n)\n\ntrainer.fit(model, train_loader, val_loader)\n</code></pre>"},{"location":"examples/finetuning/#saving-and-loading-fine-tuned-models","title":"Saving and Loading Fine-tuned Models","text":""},{"location":"examples/finetuning/#saving-a-fine-tuned-model","title":"Saving a Fine-tuned Model","text":"<pre><code>from transformers import AutoModel\nfrom tokenizer import Tokenizer\n\n# After training\nmodel = ...  # Your trained model\ntokenizer = ...  # Your tokenizer\n\n# Save model and tokenizer\nmodel.save_pretrained(\"models/my_finetuned_model\")\ntokenizer.save(\"models/my_finetuned_model/tokenizer.model\")\n\n# Also save the configuration\nimport json\nconfig = {\n    \"model_type\": \"bert\",\n    \"num_labels\": 2,\n    \"id2label\": {0: \"NEGATIVE\", 1: \"POSITIVE\"},\n    \"label2id\": {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n}\n\nwith open(\"models/my_finetuned_model/config.json\", \"w\") as f:\n    json.dump(config, f)\n</code></pre>"},{"location":"examples/finetuning/#loading-a-fine-tuned-model","title":"Loading a Fine-tuned Model","text":"<pre><code>from transformers import AutoModelForSequenceClassification\nfrom tokenizer import Tokenizer\n\n# Load model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"models/my_finetuned_model\")\ntokenizer = Tokenizer.load(\"models/my_finetuned_model/tokenizer.model\")\n\n# Example inference\ntext = \"This is a positive example.\"\ninputs = tokenizer.encode(text, return_tensors=\"pt\")\noutputs = model(**inputs)\npredictions = torch.softmax(outputs.logits, dim=1)\n</code></pre>"},{"location":"examples/finetuning/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Learn the basics of using the tokenizer</li> <li>Advanced Usage - Explore advanced features</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"examples/training/","title":"Training Examples","text":"<p>This guide provides examples of training custom tokenizers using different algorithms and configurations.</p>"},{"location":"examples/training/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Training a BPE Tokenizer</li> <li>Training a WordPiece Tokenizer</li> <li>Training a Unigram Tokenizer</li> <li>Training on Multiple Files</li> <li>Incremental Training</li> <li>Custom Training Callbacks</li> </ul>"},{"location":"examples/training/#training-a-bpe-tokenizer","title":"Training a BPE Tokenizer","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Define configuration\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=30000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n    lowercase=True,\n    max_length=512\n)\n\n# Initialize tokenizer\ntokenizer = Tokenizer(config=config)\n\n# Train on a text file\ntokenizer.train(\"data/train.txt\", vocab_size=30000)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/bpe_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#training-a-wordpiece-tokenizer","title":"Training a WordPiece Tokenizer","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Define configuration\nconfig = TokenizerConfig(\n    algorithm=\"wordpiece\",\n    vocab_size=30000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n    lowercase=True,\n    wordpieces_prefix=\"##\"\n)\n\n# Initialize tokenizer\ntokenizer = Tokenizer(config=config)\n\n# Train on a text file\ntokenizer.train(\"data/train.txt\", vocab_size=30000)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/wordpiece_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#training-a-unigram-tokenizer","title":"Training a Unigram Tokenizer","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Define configuration\nconfig = TokenizerConfig(\n    algorithm=\"unigram\",\n    vocab_size=30000,\n    unk_token=\"[UNK]\",\n    special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n    shrinking_factor=0.75\n)\n\n# Initialize tokenizer\ntokenizer = Tokenizer(config=config)\n\n# Train on a text file\ntokenizer.train(\"data/train.txt\", vocab_size=30000)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/unigram_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#training-on-multiple-files","title":"Training on Multiple Files","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# List of training files\ntraining_files = [\n    \"data/train_part1.txt\",\n    \"data/train_part2.txt\",\n    \"data/additional_data.txt\"\n]\n\n# Initialize tokenizer\ntokenizer = Tokenizer()\n\n# Train on multiple files\ntokenizer.train(\n    files=training_files,\n    vocab_size=50000,\n    min_frequency=2,\n    show_progress=True\n)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/multi_file_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#incremental-training","title":"Incremental Training","text":"<pre><code>from tokenizer import Tokenizer\n\n# Load existing tokenizer\ntokenizer = Tokenizer.load(\"models/pretrained_tokenizer.model\")\n\n# Continue training with new data\ntokenizer.train(\n    \"data/new_data.txt\",\n    vocab_size=55000,  # Optionally increase vocabulary size\n    min_frequency=2,\n    show_progress=True\n)\n\n# Save the updated tokenizer\ntokenizer.save(\"models/updated_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#custom-training-callbacks","title":"Custom Training Callbacks","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Define callbacks\nclass TrainingCallbacks:\n    def on_epoch_begin(self, epoch, logs=None):\n        print(f\"Starting epoch {epoch}\")\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"Finished epoch {epoch}. Vocab size: {logs.get('vocab_size')}\")\n\n    def on_batch_end(self, batch, logs=None):\n        if batch % 1000 == 0:\n            print(f\"Processed {batch} batches\")\n\n# Initialize tokenizer with callbacks\ntokenizer = Tokenizer()\ncallbacks = TrainingCallbacks()\n\n# Train with callbacks\ntokenizer.train(\n    \"data/large_corpus.txt\",\n    vocab_size=50000,\n    callbacks=callbacks,\n    batch_size=1000\n)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/callback_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#training-with-custom-preprocessing","title":"Training with Custom Preprocessing","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\nimport re\n\ndef custom_preprocessor(text):\n    # Custom preprocessing function\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    return text\n\n# Initialize tokenizer with custom preprocessor\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=30000,\n    preprocessor=custom_preprocessor\n)\n\ntokenizer = Tokenizer(config=config)\n\n# Train with custom preprocessing\ntokenizer.train(\"data/raw_text.txt\")\n\n# Save the trained tokenizer\ntokenizer.save(\"models/preprocessed_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#training-with-limited-resources","title":"Training with Limited Resources","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Configure for limited memory usage\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=20000,  # Smaller vocabulary\n    memory_limit=\"2GB\",  # Limit memory usage\n    batch_size=1000,     # Smaller batch size\n    lowercase=True\n)\n\ntokenizer = Tokenizer(config=config)\n\n# Train with limited resources\ntokenizer.train(\n    \"data/large_corpus.txt\",\n    show_progress=True\n)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/lightweight_tokenizer.model\")\n</code></pre>"},{"location":"examples/training/#next-steps","title":"Next Steps","text":"<ul> <li>Basic Usage - Learn the basics of using the tokenizer</li> <li>Advanced Usage - Explore advanced features</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>This guide will help you get started with MyTokenizer quickly. We'll cover the basic usage patterns and common operations.</p>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#importing-the-tokenizer","title":"Importing the Tokenizer","text":"<pre><code>from tokenizer import Tokenizer\n</code></pre>"},{"location":"getting-started/quickstart/#initializing-the-tokenizer","title":"Initializing the Tokenizer","text":"<p>Create a tokenizer instance with default settings:</p> <pre><code>tokenizer = Tokenizer()\n</code></pre>"},{"location":"getting-started/quickstart/#tokenizing-text","title":"Tokenizing Text","text":"<p>Tokenize a simple sentence:</p> <pre><code>text = \"Hello, world! This is MyTokenizer in action.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# Output: ['Hello', ',', 'world', '!', 'This', 'is', 'My', '##Token', '##izer', 'in', 'action', '.']\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch Processing","text":"<p>Process multiple texts at once:</p> <pre><code>texts = [\n    \"First sentence to tokenize.\",\n    \"Second sentence for demonstration.\",\n    \"Third example shows batch processing.\"\n]\n\nall_tokens = [tokenizer.tokenize(text) for text in texts]\nprint(all_tokens)\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-different-algorithms","title":"Working with Different Algorithms","text":""},{"location":"getting-started/quickstart/#using-bpe-byte-pair-encoding","title":"Using BPE (Byte Pair Encoding)","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=30000,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\nbpe_tokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"getting-started/quickstart/#using-wordpiece","title":"Using WordPiece","text":"<pre><code>config = TokenizerConfig(\n    algorithm=\"wordpiece\",\n    vocab_size=30000,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\nwordpiece_tokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"getting-started/quickstart/#training-your-own-tokenizer","title":"Training Your Own Tokenizer","text":""},{"location":"getting-started/quickstart/#prepare-training-data","title":"Prepare Training Data","text":"<p>Create a text file with your training data (one sentence per line):</p> <pre><code>with open(\"training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"This is the first training sentence.\\n\")\n    f.write(\"Here's another example for the tokenizer.\\n\")\n    # Add more training data...\n</code></pre>"},{"location":"getting-started/quickstart/#train-the-tokenizer","title":"Train the Tokenizer","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=5000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\ntokenizer = Tokenizer(config=config)\ntokenizer.train(\"training_data.txt\")\n</code></pre>"},{"location":"getting-started/quickstart/#saving-and-loading","title":"Saving and Loading","text":""},{"location":"getting-started/quickstart/#save-the-tokenizer","title":"Save the Tokenizer","text":"<pre><code>tokenizer.save(\"my_tokenizer.model\")\n</code></pre>"},{"location":"getting-started/quickstart/#load-the-tokenizer","title":"Load the Tokenizer","text":"<pre><code>from tokenizer import Tokenizer\n\nloaded_tokenizer = Tokenizer.load(\"my_tokenizer.model\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Advanced Usage for more features</li> <li>Check out the API Reference for detailed documentation</li> </ul>"},{"location":"guide/advanced-usage/","title":"Advanced Usage","text":"<p>This guide covers advanced features and techniques for using MyTokenizer effectively.</p>"},{"location":"guide/advanced-usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Custom Tokenization</li> <li>Training Custom Models</li> <li>Performance Optimization</li> <li>Custom Preprocessing</li> <li>Parallel Processing</li> <li>Memory Management</li> </ul>"},{"location":"guide/advanced-usage/#custom-tokenization","title":"Custom Tokenization","text":""},{"location":"guide/advanced-usage/#custom-tokenization-rules","title":"Custom Tokenization Rules","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\nimport re\n\ndef custom_tokenizer(text):\n    # Split on whitespace and punctuation, but keep email addresses intact\n    pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b|\\w+|\\S\"\n    return re.findall(pattern, text)\n\nconfig = TokenizerConfig(\n    custom_tokenizer=custom_tokenizer,\n    lowercase=True\n)\n\ntokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"guide/advanced-usage/#handling-special-cases","title":"Handling Special Cases","text":"<pre><code># Add special cases to the tokenizer\ntokenizer.add_special_case(\"gonna\", [\"gon\", \"na\"])\ntokenizer.add_special_case(\"wanna\", [\"wan\", \"na\"])\n\n# Now these will be tokenized as specified\ntokens = tokenizer.tokenize(\"I'm gonna use this tokenizer\")\n# Output: [\"I\", \"'\", \"m\", \"gon\", \"na\", \"use\", \"this\", \"token\", \"##izer\"]\n</code></pre>"},{"location":"guide/advanced-usage/#training-custom-models","title":"Training Custom Models","text":""},{"location":"guide/advanced-usage/#training-on-custom-data","title":"Training on Custom Data","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\n# Prepare your training data\ncorpus_files = [\"data/train.txt\", \"data/additional.txt\"]\n\n# Configure training\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=50000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# Initialize and train\ntokenizer = Tokenizer(config=config)\ntokenizer.train(\n    files=corpus_files,\n    vocab_size=50000,\n    min_frequency=2,\n    show_progress=True\n)\n\n# Save the trained tokenizer\ntokenizer.save(\"models/custom_tokenizer.model\")\n</code></pre>"},{"location":"guide/advanced-usage/#incremental-training","title":"Incremental Training","text":"<pre><code># Load existing tokenizer\ntokenizer = Tokenizer.load(\"models/pretrained.model\")\n\n# Continue training with new data\ntokenizer.train(\n    files=[\"data/new_data.txt\"],\n    vocab_size=55000,  # Optionally increase vocabulary size\n    min_frequency=2\n)\n</code></pre>"},{"location":"guide/advanced-usage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guide/advanced-usage/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple texts efficiently\ntexts = [\"Text 1\", \"Text 2\", ...]  # Large list of texts\nbatch_size = 1000\n\n# Process in batches\nall_tokens = []\nfor i in range(0, len(texts), batch_size):\n    batch = texts[i:i + batch_size]\n    tokens = tokenizer.tokenize_batch(batch)\n    all_tokens.extend(tokens)\n</code></pre>"},{"location":"guide/advanced-usage/#caching","title":"Caching","text":"<pre><code># Enable caching for repeated texts\ntokenizer.enable_cache(max_size=10000)  # Cache up to 10,000 unique texts\n\n# First call is slower (computes tokens)\ntokens1 = tokenizer.tokenize(\"This text will be cached.\")\n\n# Subsequent calls are faster (uses cache)\ntokens2 = tokenizer.tokenize(\"This text will be cached.\")\n\n# Clear cache if needed\ntokenizer.clear_cache()\n</code></pre>"},{"location":"guide/advanced-usage/#custom-preprocessing","title":"Custom Preprocessing","text":""},{"location":"guide/advanced-usage/#custom-preprocessing-pipeline","title":"Custom Preprocessing Pipeline","text":"<pre><code>from tokenizer import Tokenizer\nimport re\n\ndef custom_preprocessor(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    # Remove mentions and hashtags\n    text = re.sub(r'(@\\w+|#\\w+)', '', text)\n    return text.strip()\n\n# Initialize with custom preprocessor\ntokenizer = Tokenizer(preprocessor=custom_preprocessor)\n</code></pre>"},{"location":"guide/advanced-usage/#using-multiple-preprocessors","title":"Using Multiple Preprocessors","text":"<pre><code>def remove_emojis(text):\n    # Remove emojis\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport &amp; map symbols\n        \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef clean_text(text):\n    # Apply multiple cleaning steps\n    text = custom_preprocessor(text)\n    text = remove_emojis(text)\n    return text\n\ntokenizer = Tokenizer(preprocessor=clean_text)\n</code></pre>"},{"location":"guide/advanced-usage/#parallel-processing","title":"Parallel Processing","text":""},{"location":"guide/advanced-usage/#using-multiple-cores","title":"Using Multiple Cores","text":"<pre><code>from tokenizer import Tokenizer\nfrom multiprocessing import cpu_count\n\n# Initialize tokenizer with parallel processing\ntokenizer = Tokenizer(n_jobs=cpu_count())  # Use all available cores\n\n# Process large datasets in parallel\ntexts = [...]  # Large list of texts\ntokens = tokenizer.tokenize_batch(texts)  # Will use multiple cores\n</code></pre>"},{"location":"guide/advanced-usage/#memory-management","title":"Memory Management","text":""},{"location":"guide/advanced-usage/#controlling-memory-usage","title":"Controlling Memory Usage","text":"<pre><code># Limit memory usage during training\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",\n    vocab_size=50000,\n    memory_limit=\"4GB\"  # Limit memory usage to 4GB\n)\n\ntokenizer = Tokenizer(config=config)\ntokenizer.train(\"large_corpus.txt\")\n</code></pre>"},{"location":"guide/advanced-usage/#streaming-large-files","title":"Streaming Large Files","text":"<pre><code>def text_generator(file_path, batch_size=1000):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        batch = []\n        for line in f:\n            batch.append(line.strip())\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n\n# Process file in batches without loading everything into memory\nfor batch in text_generator(\"very_large_file.txt\"):\n    tokens = tokenizer.tokenize_batch(batch)\n    # Process tokens...\n</code></pre>"},{"location":"guide/advanced-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Customization - Learn how to customize tokenizer behavior</li> <li>API Reference - Detailed API documentation</li> <li>Training Examples - Examples for training tokenizers</li> <li>Fine-tuning Examples - Examples for fine-tuning models</li> </ul>"},{"location":"guide/basic-usage/","title":"Basic Usage","text":"<p>This guide covers the fundamental usage patterns of MyTokenizer.</p>"},{"location":"guide/basic-usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Tokenization</li> <li>Special Tokens</li> <li>Configuration</li> <li>Handling Different Languages</li> <li>Error Handling</li> </ul>"},{"location":"guide/basic-usage/#tokenization","title":"Tokenization","text":""},{"location":"guide/basic-usage/#basic-tokenization","title":"Basic Tokenization","text":"<pre><code>from tokenizer import Tokenizer\n\n# Initialize with default settings\ntokenizer = Tokenizer()\n\n# Tokenize a simple text\ntext = \"MyTokenizer makes tokenization easy and efficient.\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# Output: ['My', '##Token', '##izer', 'makes', 'token', '##ization', 'easy', 'and', 'efficient', '.']\n</code></pre>"},{"location":"guide/basic-usage/#batch-processing","title":"Batch Processing","text":"<p>Process multiple texts efficiently:</p> <pre><code>texts = [\n    \"First example text.\",\n    \"Second example with different length.\",\n    \"Third example for batch processing.\"\n]\n\nbatch_tokens = tokenizer.tokenize_batch(texts)\nfor i, tokens in enumerate(batch_tokens):\n    print(f\"Text {i+1}:\", tokens)\n</code></pre>"},{"location":"guide/basic-usage/#special-tokens","title":"Special Tokens","text":""},{"location":"guide/basic-usage/#using-special-tokens","title":"Using Special Tokens","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\nconfig = TokenizerConfig(\n    special_tokens={\n        \"unk_token\": \"[UNK]\",\n        \"pad_token\": \"[PAD]\",\n        \"cls_token\": \"[CLS]\",\n        \"sep_token\": \"[SEP]\",\n        \"mask_token\": \"[MASK]\"\n    }\n)\n\ntokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"guide/basic-usage/#adding-special-tokens-after-initialization","title":"Adding Special Tokens After Initialization","text":"<pre><code>tokenizer.add_special_tokens([\"[NEW1]\", \"[NEW2]\"])\n</code></pre>"},{"location":"guide/basic-usage/#configuration","title":"Configuration","text":""},{"location":"guide/basic-usage/#custom-configuration","title":"Custom Configuration","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\nconfig = TokenizerConfig(\n    algorithm=\"bpe\",          # or \"wordpiece\", \"unigram\"\n    vocab_size=30000,\n    min_frequency=2,\n    lowercase=True,\n    strip_accents=True,\n    max_length=512,\n    truncation=True,\n    padding=\"max_length\"\n)\n\ntokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"guide/basic-usage/#available-configuration-options","title":"Available Configuration Options","text":"Parameter Type Default Description algorithm str \"bpe\" Tokenization algorithm (bpe/wordpiece/unigram) vocab_size int 30000 Maximum size of the vocabulary min_frequency int 2 Minimum frequency for a token to be included lowercase bool True Whether to convert text to lowercase strip_accents bool True Whether to strip accents max_length int 512 Maximum sequence length truncation bool True Whether to truncate sequences padding str \"max_length\" Padding strategy special_tokens dict {} Special tokens configuration"},{"location":"guide/basic-usage/#handling-different-languages","title":"Handling Different Languages","text":""},{"location":"guide/basic-usage/#multilingual-tokenization","title":"Multilingual Tokenization","text":"<pre><code># Initialize with language-specific settings\nconfig = TokenizerConfig(\n    language=\"en\",  # Supports multiple languages\n    lowercase=True,\n    strip_accents=True\n)\n\ntokenizer = Tokenizer(config=config)\n\n# Tokenize text in different languages\ntext_en = \"This is an English text.\"\ntext_es = \"Este es un texto en espa\u00f1ol.\"\n\ntokens_en = tokenizer.tokenize(text_en)\ntokens_es = tokenizer.tokenize(text_es)\n</code></pre>"},{"location":"guide/basic-usage/#error-handling","title":"Error Handling","text":""},{"location":"guide/basic-usage/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>try:\n    tokens = tokenizer.tokenize(None)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"guide/basic-usage/#handling-unknown-tokens","title":"Handling Unknown Tokens","text":"<pre><code>text = \"This word_is_unknown_to_the_tokenizer\"\ntokens = tokenizer.tokenize(text, handle_unknown=True)\n# Unknown words will be replaced with the unknown token\n</code></pre>"},{"location":"guide/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Usage - Learn about advanced features</li> <li>Customization - Customize tokenizer behavior</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guide/customization/","title":"Customization Guide","text":"<p>This guide explains how to customize MyTokenizer to fit your specific needs.</p>"},{"location":"guide/customization/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Custom Tokenization Rules</li> <li>Custom Vocabulary</li> <li>Special Tokens</li> <li>Custom Preprocessing</li> <li>Custom Postprocessing</li> <li>Extending the Tokenizer</li> </ul>"},{"location":"guide/customization/#custom-tokenization-rules","title":"Custom Tokenization Rules","text":""},{"location":"guide/customization/#using-regular-expressions","title":"Using Regular Expressions","text":"<pre><code>import re\nfrom tokenizer import Tokenizer\n\n# Define custom tokenization pattern\npattern = r\"\\b\\w+\\b|\\S\"  # Words or non-whitespace characters\n\n# Create tokenizer with custom regex\ntokenizer = Tokenizer(tokenization_pattern=pattern)\n</code></pre>"},{"location":"guide/customization/#custom-tokenizer-function","title":"Custom Tokenizer Function","text":"<pre><code>def custom_tokenizer(text):\n    # Simple whitespace tokenizer with special handling for contractions\n    tokens = []\n    for word in text.split():\n        if \"'\" in word:\n            # Split contractions like \"don't\" -&gt; [\"do\", \"n't\"]\n            parts = word.split(\"'\")\n            tokens.extend(parts[:-1])\n            tokens.append(\"'\" + parts[-1])\n        else:\n            tokens.append(word)\n    return tokens\n\n# Initialize with custom tokenizer\ntokenizer = Tokenizer(custom_tokenizer=custom_tokenizer)\n</code></pre>"},{"location":"guide/customization/#custom-vocabulary","title":"Custom Vocabulary","text":""},{"location":"guide/customization/#creating-a-custom-vocabulary","title":"Creating a Custom Vocabulary","text":"<pre><code>from tokenizer import Tokenizer, Vocabulary\n\n# Create a vocabulary from a list of tokens\ncustom_vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \n               \"hello\", \"world\", \"token\", \"##ization\"]\n\n# Initialize vocabulary\nvocab = Vocabulary(custom_vocab)\n\n# Create tokenizer with custom vocabulary\ntokenizer = Tokenizer(vocab=vocab)\n</code></pre>"},{"location":"guide/customization/#updating-vocabulary","title":"Updating Vocabulary","text":"<pre><code># Add new tokens to existing vocabulary\ntokenizer.vocab.add_tokens([\"new\", \"tokens\", \"to\", \"add\"])\n\n# Remove tokens\n# Note: Be careful as this might affect existing tokenization\ntokenizer.vocab.remove_tokens([\"obsolete\", \"tokens\"])\n</code></pre>"},{"location":"guide/customization/#special-tokens","title":"Special Tokens","text":""},{"location":"guide/customization/#defining-special-tokens","title":"Defining Special Tokens","text":"<pre><code>from tokenizer import Tokenizer, TokenizerConfig\n\nconfig = TokenizerConfig(\n    special_tokens={\n        \"unk_token\": \"[UNK]\",\n        \"pad_token\": \"[PAD]\",\n        \"cls_token\": \"[CLS]\",\n        \"sep_token\": \"[SEP]\",\n        \"mask_token\": \"[MASK]\",\n        \"bos_token\": \"[BOS]\",\n        \"eos_token\": \"[EOS]\"\n    }\n)\n\ntokenizer = Tokenizer(config=config)\n</code></pre>"},{"location":"guide/customization/#adding-special-cases","title":"Adding Special Cases","text":"<pre><code># Add special cases that should be tokenized in a specific way\ntokenizer.add_special_case(\"gonna\", [\"gon\", \"na\"])\ntokenizer.add_special_case(\"wanna\", [\"wan\", \"na\"])\ntokenizer.add_special_case(\"can't\", [\"can\", \"n't\"])\n\n# Now these will be tokenized as specified\ntokens = tokenizer.tokenize(\"I'm gonna use this tokenizer\")\n# Output: [\"I\", \"'\", \"m\", \"gon\", \"na\", \"use\", \"this\", \"token\", \"##izer\"]\n</code></pre>"},{"location":"guide/customization/#custom-preprocessing","title":"Custom Preprocessing","text":""},{"location":"guide/customization/#preprocessing-pipeline","title":"Preprocessing Pipeline","text":"<pre><code>def custom_preprocessor(text):\n    \"\"\"Custom preprocessing function.\"\"\"\n    import re\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n\n    # Remove mentions and hashtags\n    text = re.sub(r'(@\\w+|#\\w+)', '', text)\n\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n\n    return text\n\n# Initialize with custom preprocessor\ntokenizer = Tokenizer(preprocessor=custom_preprocessor)\n</code></pre>"},{"location":"guide/customization/#multiple-preprocessors","title":"Multiple Preprocessors","text":"<pre><code>from functools import reduce\n\ndef compose(*functions):\n    \"\"\"Compose multiple functions into a single function.\"\"\"\n    return reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n\ndef remove_punctuation(text):\n    import string\n    return text.translate(str.maketrans('', '', string.punctuation))\n\ndef remove_numbers(text):\n    return ''.join([i for i in text if not i.isdigit()])\n\n# Combine multiple preprocessing functions\npreprocessing_pipeline = compose(\n    custom_preprocessor,\n    remove_punctuation,\n    remove_numbers\n)\n\ntokenizer = Tokenizer(preprocessor=preprocessing_pipeline)\n</code></pre>"},{"location":"guide/customization/#custom-postprocessing","title":"Custom Postprocessing","text":""},{"location":"guide/customization/#postprocessing-tokens","title":"Postprocessing Tokens","text":"<pre><code>def custom_postprocessor(tokens):\n    \"\"\"Custom postprocessing of tokens.\"\"\"\n    # Remove empty strings\n    tokens = [token for token in tokens if token.strip()]\n\n    # Merge certain tokens\n    i = 0\n    while i &lt; len(tokens) - 1:\n        if tokens[i] == \"##\" and i + 1 &lt; len(tokens):\n            tokens[i:i+2] = [tokens[i] + tokens[i+1]]\n        else:\n            i += 1\n\n    return tokens\n\n# Initialize with custom postprocessor\ntokenizer = Tokenizer(postprocessor=custom_postprocessor)\n</code></pre>"},{"location":"guide/customization/#extending-the-tokenizer","title":"Extending the Tokenizer","text":""},{"location":"guide/customization/#creating-a-custom-tokenizer-class","title":"Creating a Custom Tokenizer Class","text":"<pre><code>from tokenizer import Tokenizer\n\nclass CustomTokenizer(Tokenizer):\n    \"\"\"Custom tokenizer with additional functionality.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Custom initialization\n        self.custom_param = kwargs.pop('custom_param', None)\n        super().__init__(*args, **kwargs)\n\n    def tokenize(self, text, **kwargs):\n        # Custom tokenization logic\n        if self.custom_param:\n            # Apply custom preprocessing\n            text = self._preprocess_with_custom_param(text)\n\n        # Call parent's tokenize method\n        tokens = super().tokenize(text, **kwargs)\n\n        # Apply custom postprocessing\n        return self._custom_postprocess(tokens)\n\n    def _preprocess_with_custom_param(self, text):\n        # Custom preprocessing logic\n        return text\n\n    def _custom_postprocess(self, tokens):\n        # Custom postprocessing logic\n        return tokens\n\n# Use the custom tokenizer\ncustom_tokenizer = CustomTokenizer(custom_param=\"value\")\n</code></pre>"},{"location":"guide/customization/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Usage - Learn about advanced features</li> <li>API Reference - Detailed API documentation</li> <li>Training Examples - Examples for training tokenizers</li> <li>Fine-tuning Examples - Examples for fine-tuning models</li> </ul>"}]}